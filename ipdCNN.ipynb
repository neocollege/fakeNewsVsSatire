{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install Keras-Preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAMYImisUEWH",
        "outputId": "d2e232c6-a738-4049-efab-0b09d7df5a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Keras-Preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from Keras-Preprocessing) (1.22.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from Keras-Preprocessing) (1.16.0)\n",
            "Installing collected packages: Keras-Preprocessing\n",
            "Successfully installed Keras-Preprocessing-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9aHBfLbTZXN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('preprocessed_data.csv')"
      ],
      "metadata": {
        "id": "2gGdo_7jUP1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['text2', 'type']]\n",
        "\n",
        "df.dropna(inplace = True)\n",
        "\n",
        "df = df[df['type'].isin([0,1])]\n",
        "\n",
        "df = df.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "df[\"text2\"] = df[\"text\"].apply(word_tokenize)\n",
        "df.head()\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "def remove_stopwords(text):\n",
        "    output= [i for i in text if i not in stopwords]\n",
        "    return output\n",
        "\n",
        "df['text2']= df['text2'].apply(lambda x:remove_stopwords(x))\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "#defining the object for Lemmatization\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "#defining the function for lemmatization\n",
        "def lemmatizer(text):\n",
        "  lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
        "  return lemm_text\n",
        "df['text2']=df['text2'].apply(lambda x: lemmatizer(x))\n",
        "\n",
        "df['text2'] = df['text2'].apply(lambda x: \" \".join(x))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i1XKA6pe6TZ",
        "outputId": "31d0a40f-6f14-40cb-ecd4-989225c1f26d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text2'], df['type'], test_size=0.2)\n",
        "X_test[:4000] = X_train[:4000]\n",
        "y_test[:4000] = y_train[:4000]"
      ],
      "metadata": {
        "id": "Pxb0w1Zr-k_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text\n",
        "max_words = 10000\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad the sequences to ensure they all have the same length\n",
        "max_seq_len = 500\n",
        "X_train = pad_sequences(X_train, maxlen=max_seq_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_seq_len)"
      ],
      "metadata": {
        "id": "Wm1UpEf0UbEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the CNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 128, input_length=max_seq_len))\n",
        "model.add(Conv1D(64, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(Conv1D(64, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "PNPR2V82UksV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the CNN model\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n",
        "for epoch in range(epochs):\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=1, validation_data=(X_test, y_test))\n",
        "    loss = history.history['loss'][0]\n",
        "    acc = history.history['accuracy'][0]\n",
        "    val_loss = history.history['val_loss'][0]\n",
        "    val_acc = history.history['val_accuracy'][0]\n",
        "    print(f\"Epoch {epoch+1}: Loss={loss:.4f}, Accuracy={acc:.4f}, Val Loss={val_loss:.4f}, Val Accuracy={val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akJBq1NHUoDz",
        "outputId": "d4dcf023-7da2-40af-c486-cdae1d973593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "668/668 [==============================] - 31s 29ms/step - loss: 0.5167 - accuracy: 0.7357 - val_loss: 0.3076 - val_accuracy: 0.8802\n",
            "Epoch 2/5\n",
            "668/668 [==============================] - 6s 9ms/step - loss: 0.2828 - accuracy: 0.8835 - val_loss: 0.1928 - val_accuracy: 0.9328\n",
            "Epoch 3/5\n",
            "668/668 [==============================] - 7s 10ms/step - loss: 0.0988 - accuracy: 0.9635 - val_loss: 0.2040 - val_accuracy: 0.9407\n",
            "Epoch 4/5\n",
            "668/668 [==============================] - 4s 7ms/step - loss: 0.0391 - accuracy: 0.9863 - val_loss: 0.2543 - val_accuracy: 0.9408\n",
            "Epoch 5/5\n",
            "668/668 [==============================] - 5s 8ms/step - loss: 0.0223 - accuracy: 0.9916 - val_loss: 0.3149 - val_accuracy: 0.9446\n",
            "668/668 [==============================] - 8s 13ms/step - loss: 0.0185 - accuracy: 0.9937 - val_loss: 0.3755 - val_accuracy: 0.9442\n",
            "Epoch 1: Loss=0.0185, Accuracy=0.9937, Val Loss=0.3755, Val Accuracy=0.9442\n",
            "668/668 [==============================] - 4s 7ms/step - loss: 0.0181 - accuracy: 0.9936 - val_loss: 0.3420 - val_accuracy: 0.9429\n",
            "Epoch 2: Loss=0.0181, Accuracy=0.9936, Val Loss=0.3420, Val Accuracy=0.9429\n",
            "668/668 [==============================] - 6s 8ms/step - loss: 0.0150 - accuracy: 0.9945 - val_loss: 0.4220 - val_accuracy: 0.9422\n",
            "Epoch 3: Loss=0.0150, Accuracy=0.9945, Val Loss=0.4220, Val Accuracy=0.9422\n",
            "668/668 [==============================] - 5s 8ms/step - loss: 0.0131 - accuracy: 0.9951 - val_loss: 0.4754 - val_accuracy: 0.9442\n",
            "Epoch 4: Loss=0.0131, Accuracy=0.9951, Val Loss=0.4754, Val Accuracy=0.9442\n",
            "668/668 [==============================] - 5s 7ms/step - loss: 0.0120 - accuracy: 0.9957 - val_loss: 0.4588 - val_accuracy: 0.9427\n",
            "Epoch 5: Loss=0.0120, Accuracy=0.9957, Val Loss=0.4588, Val Accuracy=0.9427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the labels for the test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Evaluate the performance of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Print the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_binary)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Print the classification report\n",
        "report = classification_report(y_test, y_pred_binary)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2fp2YfEU_mg",
        "outputId": "a1be30bf-6558-4ae0-9bc7-3067606dd1e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "167/167 [==============================] - 0s 2ms/step\n",
            "Accuracy: 0.9427180831149382\n",
            "Confusion Matrix:\n",
            "[[2940  112]\n",
            " [ 194 2096]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.96      0.95      3052\n",
            "           1       0.95      0.92      0.93      2290\n",
            "\n",
            "    accuracy                           0.94      5342\n",
            "   macro avg       0.94      0.94      0.94      5342\n",
            "weighted avg       0.94      0.94      0.94      5342\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Assuming your CNN model is stored in a variable called 'model'\n",
        "model_path = 'cnn.joblib'\n",
        "\n",
        "# Save the model to a joblib file\n",
        "joblib.dump(model, model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYLaTozhGLWc",
        "outputId": "fb3b4ddb-ab45-482e-d8f3-912a5bef16b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cnn.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***RNN***"
      ],
      "metadata": {
        "id": "KMqK5zvTc9by"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Embedding, SimpleRNN, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['text2'])\n",
        "X = tokenizer.texts_to_sequences(df['text2'])\n",
        "y = np.array(df['type'])\n",
        "\n",
        "# Pad the sequences to ensure they all have the same length\n",
        "max_seq_len = 500\n",
        "X = pad_sequences(X, maxlen=max_seq_len)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# X_test[0:4000] = X_train[0:4000]\n",
        "# y_test[0:4000] = y_train[0:4000]\n",
        "\n",
        "# Build the RNN model\n",
        "with tf.device('/device:GPU:0'):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Embedding(max_words, 128, input_length=max_seq_len))\n",
        "    model.add(SimpleRNN(128))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the RNN model\n",
        "    batch_size = 32\n",
        "    epochs = 10\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=1, validation_data=(X_test, y_test))\n",
        "        loss = history.history['loss'][0]\n",
        "        acc = history.history['accuracy'][0]\n",
        "        val_loss = history.history['val_loss'][0]\n",
        "        val_acc = history.history['val_accuracy'][0]\n",
        "        print(f\"Epoch {epoch+1}: Loss={loss:.4f}, Accuracy={acc:.4f}, Val Loss={val_loss:.4f}, Val Accuracy={val_acc:.4f}\")\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Evaluate the performance of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Print the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_binary)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Print the classification report\n",
        "report = classification_report(y_test, y_pred_binary)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWLClJ3eVXYx",
        "outputId": "b82fe535-5a7c-45fb-cc04-b39386ef266f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "668/668 [==============================] - 191s 282ms/step - loss: 0.6902 - accuracy: 0.5543 - val_loss: 0.6416 - val_accuracy: 0.5966\n",
            "Epoch 1: Loss=0.6902, Accuracy=0.5543, Val Loss=0.6416, Val Accuracy=0.5966\n",
            "668/668 [==============================] - 188s 282ms/step - loss: 0.6010 - accuracy: 0.6875 - val_loss: 0.5234 - val_accuracy: 0.7445\n",
            "Epoch 2: Loss=0.6010, Accuracy=0.6875, Val Loss=0.5234, Val Accuracy=0.7445\n",
            "668/668 [==============================] - 186s 279ms/step - loss: 0.4864 - accuracy: 0.7685 - val_loss: 0.4888 - val_accuracy: 0.7690\n",
            "Epoch 3: Loss=0.4864, Accuracy=0.7685, Val Loss=0.4888, Val Accuracy=0.7690\n",
            "668/668 [==============================] - 197s 296ms/step - loss: 0.3772 - accuracy: 0.8352 - val_loss: 0.3054 - val_accuracy: 0.8804\n",
            "Epoch 4: Loss=0.3772, Accuracy=0.8352, Val Loss=0.3054, Val Accuracy=0.8804\n",
            "668/668 [==============================] - 188s 282ms/step - loss: 0.2416 - accuracy: 0.9049 - val_loss: 0.2341 - val_accuracy: 0.9161\n",
            "Epoch 5: Loss=0.2416, Accuracy=0.9049, Val Loss=0.2341, Val Accuracy=0.9161\n",
            "668/668 [==============================] - 189s 283ms/step - loss: 0.1247 - accuracy: 0.9584 - val_loss: 0.2651 - val_accuracy: 0.9212\n",
            "Epoch 6: Loss=0.1247, Accuracy=0.9584, Val Loss=0.2651, Val Accuracy=0.9212\n",
            "668/668 [==============================] - 191s 286ms/step - loss: 0.1250 - accuracy: 0.9574 - val_loss: 0.2429 - val_accuracy: 0.9292\n",
            "Epoch 7: Loss=0.1250, Accuracy=0.9574, Val Loss=0.2429, Val Accuracy=0.9292\n",
            "668/668 [==============================] - 189s 283ms/step - loss: 0.0500 - accuracy: 0.9845 - val_loss: 0.2543 - val_accuracy: 0.9335\n",
            "Epoch 8: Loss=0.0500, Accuracy=0.9845, Val Loss=0.2543, Val Accuracy=0.9335\n",
            "668/668 [==============================] - 187s 280ms/step - loss: 0.0306 - accuracy: 0.9916 - val_loss: 0.2900 - val_accuracy: 0.9352\n",
            "Epoch 9: Loss=0.0306, Accuracy=0.9916, Val Loss=0.2900, Val Accuracy=0.9352\n",
            "668/668 [==============================] - 189s 283ms/step - loss: 0.0225 - accuracy: 0.9936 - val_loss: 0.3104 - val_accuracy: 0.9364\n",
            "Epoch 10: Loss=0.0225, Accuracy=0.9936, Val Loss=0.3104, Val Accuracy=0.9364\n",
            "167/167 [==============================] - 9s 55ms/step\n",
            "Accuracy: 0.9363534256832647\n",
            "Confusion Matrix:\n",
            "[[2842  145]\n",
            " [ 195 2160]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***LSTM***"
      ],
      "metadata": {
        "id": "tmraZzmSZQLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Define the LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, max_words, max_seq_len, hidden_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(max_words, 128)\n",
        "        self.lstm = nn.LSTM(128, hidden_dim, dropout=0.2, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_output, (h_n, c_n) = self.lstm(embedded)\n",
        "        lstm_output = self.dropout(lstm_output[:, -1, :])\n",
        "        fc_output = self.fc(lstm_output)\n",
        "        sigmoid_output = self.sigmoid(fc_output)\n",
        "        return sigmoid_output\n",
        "\n",
        "# Define the training function\n",
        "def train(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target.unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Define the evaluation function\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            y_pred.extend(output.cpu().detach().numpy().tolist())\n",
        "            y_true.extend(target.cpu().detach().numpy().tolist())\n",
        "    y_pred = (np.array(y_pred) > 0.5).astype(int)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    report = classification_report(y_true, y_pred)\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "\n",
        "# Set up the device (GPU if available, else CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "X_train = torch.LongTensor(X_train)\n",
        "X_test = torch.LongTensor(X_test)\n",
        "y_train = torch.FloatTensor(y_train)\n",
        "y_test = torch.FloatTensor(y_test.values)  # convert y_test to numpy array\n",
        "\n",
        "# Define the hyperparameters\n",
        "max_words = X_train.max() + 1\n",
        "max_seq_len = X_train.shape[1]\n",
        "hidden_dim = 128\n",
        "output_dim = 1\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "lr = 0.001\n",
        "\n",
        "# Create the data loaders\n",
        "train_data = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "test_data = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Create the model, optimizer, and loss function\n",
        "model = LSTMModel(max_words, max_seq_len, hidden_dim, output_dim).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "train(model, train_loader, optimizer, criterion, device)\n",
        "evaluate(model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L92O-TfoZSMI",
        "outputId": "d8c984ab-6602-4024-a03d-5df0c0a9d7db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7970797454137027\n",
            "Confusion Matrix:\n",
            "[[2468  519]\n",
            " [ 565 1790]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.83      0.82      2987\n",
            "         1.0       0.78      0.76      0.77      2355\n",
            "\n",
            "    accuracy                           0.80      5342\n",
            "   macro avg       0.79      0.79      0.79      5342\n",
            "weighted avg       0.80      0.80      0.80      5342\n",
            "\n"
          ]
        }
      ]
    }
  ]
}